Esta clase permanece sin cambios respecto a la última versión que te proporcioné. Es la que contiene la lógica detallada de cómo se entrena, evalúa y registra un modelo o una búsqueda de hiperparámetros.

# src/models/model_trainer.py (Este archivo NO CAMBIA)

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.compose import ColumnTransformer
import mlflow
import mlflow.sklearn
import mlflow.xgboost
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class ModelTrainer:
    """
    Clase para gestionar el entrenamiento, evaluación y registro de modelos de ML
    para el problema de Telco Churn, utilizando MLflow para el seguimiento.
    """
    def __init__(self, preprocessor: ColumnTransformer, X_train: pd.DataFrame, y_train: pd.Series,
                 X_test: pd.DataFrame, y_test: pd.Series, X_full: pd.DataFrame, y_full: pd.Series,
                 random_state: int = 42):
        self.preprocessor = preprocessor
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test
        self.X_full = X_full # Datos completos para CV
        self.y_full = y_full # Datos completos para CV
        self.random_state = random_state
        self.best_model_pipeline = None
        self.best_f1_score = -1
        self.best_model_name = ""

        # Configurar el nombre del experimento global de MLflow
        mlflow.set_experiment("Telco Churn Prediction Experiments")

    def _log_metrics(self, y_true, y_pred, y_proba, run_name: str):
        """Calcula y registra métricas comunes en MLflow."""
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        roc_auc = roc_auc_score(y_true, y_proba)

        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("f1_score", f1)
        mlflow.log_metric("roc_auc", roc_auc)
        logging.info(f"Métricas para {run_name}: Accuracy={accuracy:.4f}, F1={f1:.4f}, AUC={roc_auc:.4f}")
        return f1, roc_auc

    def train_and_evaluate(self, model_name: str, model, params: dict = None) -> tuple:
        """
        Entrena y evalúa un modelo, registrando los resultados con MLflow.
        """
        with mlflow.start_run(run_name=model_name):
            logging.info(f"Iniciando ejecución de MLflow para el modelo: {model_name}")

            # Habilitar autologging para el tipo de modelo
            if isinstance(model, RandomForestClassifier):
                mlflow.sklearn.autolog()
            elif isinstance(model, XGBClassifier):
                mlflow.xgboost.autolog()
            else:
                logging.warning(f"Autologging no configurado para el tipo de modelo: {type(model)}")

            # Crear pipeline completo: preprocesamiento + modelo
            full_pipeline = Pipeline(steps=[('preprocessor', self.preprocessor),
                                            ('classifier', model)])
            logging.info(f"Pipeline completo creado para {model_name}.")

            # Registrar parámetros adicionales si se proporcionan
            if params:
                mlflow.log_params(params)
                logging.info(f"Parámetros registrados: {params}")

            # Entrenar el modelo
            logging.info(f"Entrenando el modelo {model_name}...")
            full_pipeline.fit(self.X_train, self.y_train)
            logging.info(f"Modelo {model_name} entrenado.")

            # Realizar predicciones
            y_pred = full_pipeline.predict(self.X_test)
            y_proba = full_pipeline.predict_proba(self.X_test)[:, 1]

            # Calcular y registrar métricas
            f1, roc_auc = self._log_metrics(self.y_test, y_pred, y_proba, model_name)

            # Actualizar el mejor modelo si este es mejor
            if f1 > self.best_f1_score:
                self.best_f1_score = f1
                self.best_model_pipeline = full_pipeline
                self.best_model_name = model_name
                logging.info(f"Nuevo mejor modelo: {model_name} con F1-Score: {f1:.4f}")

            logging.info(f"Ejecución de MLflow para {model_name} finalizada.")
            return full_pipeline, f1, roc_auc

    def perform_cross_validation(self, model_name: str, model, cv: int = 5) -> float:
        """
        Realiza validación cruzada y registra las métricas promedio con MLflow.
        """
        # Usamos un run separado para CV para mantener los experimentos limpios
        with mlflow.start_run(run_name=f"{model_name}_CV"):
            logging.info(f"Iniciando ejecución de MLflow para Validación Cruzada de: {model_name}")

            full_pipeline = Pipeline(steps=[('preprocessor', self.preprocessor),
                                            ('classifier', model)])

            cv_results = cross_val_score(full_pipeline, self.X_full, self.y_full, cv=cv, scoring='f1', n_jobs=-1)
            logging.info(f"Resultados de CV (F1-Score) para {model_name}: {cv_results}")

            mlflow.log_metric("cv_f1_mean", cv_results.mean())
            mlflow.log_metric("cv_f1_std", cv_results.std())
            logging.info(f"CV F1-Score promedio: {cv_results.mean():.4f} +/- {cv_results.std():.4f}")

            logging.info(f"Ejecución de MLflow para CV de {model_name} finalizada.")
            return cv_results.mean()

    def run_grid_search(self, model_name: str, base_model, param_grid: dict) -> tuple:
        """
        Ejecuta GridSearchCV y registra el mejor modelo con MLflow.
        """
        with mlflow.start_run(run_name=f"{model_name}_GridSearch"):
            mlflow.sklearn.autolog() # Autologging para sklearn

            logging.info(f"Iniciando GridSearchCV para {model_name}...")
            grid_pipeline = Pipeline(steps=[('preprocessor', self.preprocessor),
                                             ('classifier', base_model)])
            grid_search = GridSearchCV(grid_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=1)
            grid_search.fit(self.X_train, self.y_train)
            logging.info(f"GridSearchCV para {model_name} finalizado.")

            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
            logging.info(f"Mejores parámetros para {model_name}: {best_params}")

            y_pred = best_model.predict(self.X_test)
            y_proba = best_model.predict_proba(self.X_test)[:, 1]

            f1, roc_auc = self._log_metrics(self.y_test, y_pred, y_proba, f"Best {model_name} (GridSearch)")

            # Actualizar el mejor modelo si este es mejor
            if f1 > self.best_f1_score:
                self.best_f1_score = f1
                self.best_model_pipeline = best_model
                self.best_model_name = f"{model_name}_GridSearch_Best"
                logging.info(f"Nuevo mejor modelo: {self.best_model_name} con F1-Score: {f1:.4f}")

            return best_model, f1, roc_auc

    def register_final_best_model(self):
        """
        Registra el mejor modelo global encontrado en el MLflow Model Registry.
        """
        if self.best_model_pipeline is None:
            logging.warning("No hay un mejor modelo para registrar. Ejecuta los experimentos primero.")
            return

        logging.info(f"Registrando el mejor modelo global '{self.best_model_name}' (F1: {self.best_f1_score:.4f}) en el MLflow Model Registry.")
        mlflow.sklearn.log_model(
            sk_model=self.best_model_pipeline,
            artifact_path="telco_churn_model",
            registered_model_name="TelcoChurnPredictor",
            signature=mlflow.models.infer_signature(self.X_train, self.best_model_pipeline.predict(self.X_train)),
            input_example=self.X_train.head(2)
        )
        logging.info("Modelo 'TelcoChurnPredictor' registrado en el Model Registry.")


Este script ahora contendrá funciones específicas para cada experimento, que a su vez llamarán a los métodos de la clase ModelTrainer.

# src/models/run_experiments.py

import logging
from src.data.data_processor import TelcoDataProcessor
from src.models.model_trainer import ModelTrainer
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Funciones para cada experimento ---

def run_random_forest_baseline(model_trainer: ModelTrainer, random_state: int):
    """
    Ejecuta el experimento de Random Forest Baseline.
    """
    logging.info("\n--- Ejecutando Experimento 1: Random Forest Baseline ---")
    rf_model = RandomForestClassifier(random_state=random_state)
    rf_params = {'n_estimators': 100, 'max_depth': 10, 'min_samples_leaf': 5}
    rf_model.set_params(**rf_params)
    model_trainer.train_and_evaluate("RandomForest_Baseline", rf_model, rf_params)
    model_trainer.perform_cross_validation("RandomForest_Baseline", rf_model)

def run_xgboost_baseline(model_trainer: ModelTrainer, random_state: int):
    """
    Ejecuta el experimento de XGBoost Baseline.
    """
    logging.info("\n--- Ejecutando Experimento 2: XGBoost Baseline ---")
    xgb_model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=random_state)
    xgb_params = {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 5, 'subsample': 0.8}
    xgb_model.set_params(**xgb_params)
    model_trainer.train_and_evaluate("XGBoost_Baseline", xgb_model, xgb_params)
    model_trainer.perform_cross_validation("XGBoost_Baseline", xgb_model)

def run_random_forest_grid_search(model_trainer: ModelTrainer, random_state: int):
    """
    Ejecuta el experimento de Random Forest con GridSearchCV.
    """
    logging.info("\n--- Ejecutando Experimento 3: Random Forest con GridSearchCV ---")
    param_grid_rf = {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [5, 10, 15],
        'classifier__min_samples_leaf': [1, 2, 4]
    }
    model_trainer.run_grid_search("RandomForest", RandomForestClassifier(random_state=random_state), param_grid_rf)

# --- Función principal para orquestar todo ---
def main():
    """
    Función principal para orquestar el ETL, Feature Engineering y los experimentos de ML.
    """
    filepath = 'data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv'
    test_size = 0.2
    random_state = 42

    logging.info("Iniciando el pipeline completo de Telco Churn.")

    # --- 1. ETL y Feature Engineering ---
    data_processor = TelcoDataProcessor(filepath)
    df = data_processor.load_data()
    df = data_processor.initial_preprocess()
    df = data_processor.feature_engineer()
    data_processor.validate_data() # Validar datos después de FE

    # Preparar datos para el modelado y obtener el preprocesador
    X, y, preprocessor = data_processor.prepare_for_modeling()

    # Dividir datos para entrenamiento y prueba
    X_train, X_test, y_train, y_test = data_processor.split_data(test_size, random_state)

    # --- 2. Inicializar ModelTrainer ---
    model_trainer = ModelTrainer(
        preprocessor=preprocessor,
        X_train=X_train, y_train=y_train,
        X_test=X_test, y_test=y_test,
        X_full=X, y_full=y, # Usamos los datos completos para CV
        random_state=random_state
    )

    # --- 3. Ejecutar los 3 Experimentos usando las funciones ---
    run_random_forest_baseline(model_trainer, random_state)
    run_xgboost_baseline(model_trainer, random_state)
    run_random_forest_grid_search(model_trainer, random_state)

    # --- 4. Registrar el Mejor Modelo Global ---
    logging.info(f"\nMejor modelo global encontrado: {model_trainer.best_model_name} con F1-Score: {model_trainer.best_f1_score:.4f}")
    model_trainer.register_final_best_model()

    logging.info("Todos los experimentos han finalizado.")

if __name__ == "__main__":
    main()


Explicación de los Experimentos en Funciones:
Ahora, en src/models/run_experiments.py, tienes tres funciones claras:

run_random_forest_baseline(model_trainer: ModelTrainer, random_state: int):

Propósito: Entrenar y evaluar un modelo RandomForestClassifier con un conjunto de hiperparámetros base.
Pasos:
Inicializa el RandomForestClassifier.
Establece los parámetros base.
Llama a model_trainer.train_and_evaluate() para entrenar, evaluar y registrar los resultados en MLflow.
Llama a model_trainer.perform_cross_validation() para una evaluación más robusta con CV, también registrando en MLflow.
run_xgboost_baseline(model_trainer: ModelTrainer, random_state: int):

Propósito: Entrenar y evaluar un modelo XGBClassifier con un conjunto de hiperparámetros base.
Pasos:
Inicializa el XGBClassifier.
Establece los parámetros base.
Llama a model_trainer.train_and_evaluate() para entrenar, evaluar y registrar los resultados en MLflow.
Llama a model_trainer.perform_cross_validation() para una evaluación más robusta con CV, también registrando en MLflow.
run_random_forest_grid_search(model_trainer: ModelTrainer, random_state: int):

Propósito: Realizar una búsqueda de cuadrícula (GridSearchCV) para encontrar los mejores hiperparámetros para un RandomForestClassifier.
Pasos:
Define el param_grid con los rangos de hiperparámetros a explorar.
Llama a model_trainer.run_grid_search() para ejecutar la búsqueda, entrenar el mejor modelo encontrado y registrarlo en MLflow.
La función main() ahora actúa como el orquestador principal, llamando a estas funciones de experimento en secuencia después de preparar los datos.

Esta estructura te da la claridad de funciones para cada experimento, mientras que la clase ModelTrainer encapsula la lógica común de MLflow y evaluación, haciendo el código más limpio y reutilizable.
